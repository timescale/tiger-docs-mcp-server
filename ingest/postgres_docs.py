import argparse
from dataclasses import dataclass
from dotenv import load_dotenv
import base64
from bs4 import BeautifulSoup, element as BeautifulSoupElement
import json
from markdownify import markdownify
import openai
import os
from pathlib import Path
import psycopg
from psycopg.sql import SQL, Identifier
import re
import shutil
import subprocess
import tiktoken
import uuid


THIS_DIR = Path(__file__).parent.resolve()

load_dotenv(dotenv_path=os.path.join(THIS_DIR, "..", ".env"))

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

POSTGRES_DIR = THIS_DIR / "postgres"
SMGL_DIR = POSTGRES_DIR / "doc" / "src" / "sgml"
HTML_DIR = SMGL_DIR / "html"
BUILD_DIR = THIS_DIR / "build"
BUILD_DIR.mkdir(exist_ok=True)
MD_DIR = BUILD_DIR / "md"

POSTGRES_BASE_URL = "https://www.postgresql.org/docs"

ENC = tiktoken.get_encoding("cl100k_base")
MAX_CHUNK_TOKENS = 7000

TMP_ID = (
    base64.urlsafe_b64encode(uuid.uuid4().bytes)
    .rstrip(b"=")
    .decode("ascii")
    .replace("-", "_")
    .replace("+", "_")
)
TMP_CHUNKS_TABLE = SQL("{schema}.{table}").format(
    schema=Identifier("docs"), table=Identifier(f"postgres_chunks_tmp_{TMP_ID}")
)
TMP_PAGES_TABLE = SQL("{schema}.{table}").format(
    schema=Identifier("docs"), table=Identifier(f"postgres_pages_tmp_{TMP_ID}")
)


def update_repo():
    if not POSTGRES_DIR.exists():
        subprocess.run(
            "git clone https://github.com/postgres/postgres.git postgres",
            shell=True,
            check=True,
            env=os.environ,
            text=True,
        )
    else:
        subprocess.run(
            "git fetch",
            shell=True,
            check=True,
            env=os.environ,
            text=True,
            cwd=POSTGRES_DIR,
        )


def get_version_tag(version: int) -> str:
    result = subprocess.run(
        ["git", "tag", "-l"], capture_output=True, text=True, cwd=POSTGRES_DIR
    )
    if result.returncode != 0:
        raise RuntimeError("Failed to get git tags")

    tags = result.stdout.splitlines()

    candidate_tags = []

    for version_type in ["", "RC", "BETA"]:
        pattern = re.compile(rf"REL_{version}_{version_type}(\d+)$")
        for tag in tags:
            match = pattern.match(tag)
            if match:
                minor_version = int(match.group(1))
                candidate_tags.append((minor_version, tag))
        if len(candidate_tags) > 0:
            break

    if not candidate_tags:
        raise ValueError(f"No tags found for Postgres version {version}")

    candidate_tags.sort(key=lambda x: x[0], reverse=True)
    return candidate_tags[0][1]


def checkout_tag(tag: str) -> None:
    print(f"checking out {tag}...")
    subprocess.run(
        f"git checkout {tag}",
        shell=True,
        check=True,
        env=os.environ,
        text=True,
        cwd=POSTGRES_DIR,
    )


def build_html() -> None:
    html_stamp = SMGL_DIR / "html-stamp"

    # make uses the presence of html-stamp to determine if it needs to
    # rebuild the html docs.
    if html_stamp.exists():
        html_stamp.unlink()

    if HTML_DIR.exists():
        shutil.rmtree(HTML_DIR)

    print("configuring postgres build...")
    environ = os.environ.copy()
    # Shim for macOS and icu4c installed via homebrew, where it's not linked into
    # /usr/local by default.
    if Path("/opt/homebrew/opt/icu4c/lib/pkgconfig").exists():
        environ["PKG_CONFIG_PATH"] = "/opt/homebrew/opt/icu4c/lib/pkgconfig"
    subprocess.run(
        "./configure --without-readline --without-zlib",
        shell=True,
        check=True,
        env=environ,
        text=True,
        cwd=POSTGRES_DIR,
    )

    print("building postgres docs...")
    subprocess.run(
        "make html",
        shell=True,
        check=True,
        env=os.environ,
        text=True,
        cwd=SMGL_DIR,
    )


def build_markdown() -> None:
    print("converting to markdown...")
    if MD_DIR.exists():
        shutil.rmtree(MD_DIR)
    MD_DIR.mkdir()

    for html_file in HTML_DIR.glob("*.html"):
        # Skip files which are more metadata about the docs than actual docs
        # that people would ask questions about.
        if html_file.name in [
            "legalnotice.html",
            "appendix-obsolete.md",
            "appendixes.md",
            "biblio.html",
            "bookindex.html",
            "bug-reporting.html",
            "source-format.html",
            "error-message-reporting.html",
            "error-style-guide.html",
            "source-conventions.html",
            "sourcerepo.html",
        ] or html_file.name.startswith("docguide"):
            continue
        md_file = MD_DIR / (html_file.stem + ".md")

        html_content = html_file.read_text(encoding="utf-8")
        html_content = html_content.replace(
            '<?xml version="1.0" encoding="UTF-8" standalone="no"?>', ""
        )

        soup = BeautifulSoup(html_content, "html.parser")

        is_refentry = bool(soup.find("div", class_="refentry"))

        elem = soup.find("div", attrs={"id": True})
        if elem and isinstance(elem, BeautifulSoupElement.Tag):
            slug = str(elem["id"]).lower() + ".html"
        else:
            raise SystemError(f"No div with id found in {html_file}")

        title = soup.find("title")
        title_text = (
            str(title.string).strip()
            if title and isinstance(title, BeautifulSoupElement.Tag)
            else "PostgreSQL Documentation"
        )
        if title:
            title.decompose()
        for class_name in ["navheader", "navfooter"]:
            for div in soup.find_all("div", class_=class_name):
                div.decompose()

        # Don't bother including refentry in the transform as we don't chunk
        # them by headers anyway.
        if not is_refentry:
            # Convert h3 headings in admonitions to h4 so that we avoid
            # chunking them.
            for class_name in [
                "caution",
                "important",
                "notice",
                "warning",
                "tip",
                "note",
            ]:
                for div in soup.find_all("div", class_=class_name):
                    if div is None or not isinstance(div, BeautifulSoupElement.Tag):
                        continue
                    h3 = div.find("h3")
                    if h3 and isinstance(h3, BeautifulSoupElement.Tag):
                        h3.name = "h4"

        md_content = markdownify(str(soup), heading_style="ATX")
        md_content = f"""---
title: {title_text}
slug: {slug}
refentry: {is_refentry}
---
{md_content}"""
        md_file.write_text(md_content, encoding="utf-8")


@dataclass
class Page:
    id: int
    version: int
    url: str
    domain: str
    filename: str


@dataclass
class Chunk:
    idx: int
    header: str
    header_path: list[str]
    content: str
    token_count: int = 0
    subindex: int = 0


def insert_page(
    conn: psycopg.Connection,
    page: Page,
) -> None:
    print("inserting page", page.filename, page.url)
    result = conn.execute(
        SQL(
            "insert into {table} (version, url, domain, filename, content_length, chunks_count) values (%s,%s,%s,%s,%s,%s) RETURNING id"
        ).format(table=TMP_PAGES_TABLE),
        [
            page.version,
            page.url,
            page.domain,
            page.filename,
            0,
            0,
        ],
    )
    row = result.fetchone()
    assert row is not None
    page.id = row[0]


def update_page_stats(
    conn: psycopg.Connection,
    page: Page,
) -> None:
    conn.execute(
        SQL("""
        update {pages_table} p
        set
            content_length = coalesce(chunks_stats.total_length, 0),
            chunks_count = coalesce(chunks_stats.chunks_count, 0)
        from (
            select
                page_id,
                sum(char_length(content)) as total_length,
                count(*) as chunks_count
            from {chunks_table}
            where page_id = %s
            group by page_id
        ) as chunks_stats
        where p.id = chunks_stats.page_id and p.id = %s
    """).format(
            pages_table=TMP_PAGES_TABLE,
            chunks_table=TMP_CHUNKS_TABLE,
        ),
        [page.id, page.id],
    )


def insert_chunk(
    conn: psycopg.Connection,
    page: Page,
    chunk: Chunk,
) -> None:
    client = openai.OpenAI(api_key=OPENAI_API_KEY)
    content = ""
    for i in range(len(chunk.header_path)):
        content += (
            "".join(["#" for _ in range(i + 1)]) + " " + chunk.header_path[i] + "\n\n"
        )
    content += chunk.content
    embedding = (
        client.embeddings.create(
            model="text-embedding-3-small",
            input=chunk.content,
        )
        .data[0]
        .embedding
    )
    content = chunk.content
    # token_count, embedding = embed(header_path, content)
    print(f"header: {chunk.header}")
    url = page.url
    if len(chunk.header_path) > 1:
        pattern = r"\((#\S+)\)"
        match = re.search(pattern, chunk.header_path[-1])
        if match:
            url += match.group(1).lower()
    conn.execute(
        SQL(
            "insert into {table} (page_id, chunk_index, sub_chunk_index, content, metadata, embedding) values (%s,%s,%s,%s,%s,%s)"
        ).format(table=TMP_CHUNKS_TABLE),
        [
            page.id,
            chunk.idx,
            chunk.subindex,
            chunk.content,
            json.dumps(
                {
                    "header": chunk.header,
                    "header_path": chunk.header_path,
                    "source_url": url,
                    "token_count": chunk.token_count,
                }
            ),
            embedding,
        ],
    )


def split_chunk(chunk: Chunk) -> list[Chunk]:
    num_subchunks = (chunk.token_count // MAX_CHUNK_TOKENS) + 1
    input_ids = ENC.encode(chunk.content)

    tokens_per_chunk = len(input_ids) // num_subchunks

    subchunks = []
    subindex = 0
    idx = 0
    while idx < len(input_ids):
        cur_idx = min(idx + tokens_per_chunk, len(input_ids))
        chunk_ids = input_ids[idx:cur_idx]
        if not chunk_ids:
            break
        decoded = ENC.decode(chunk_ids)
        if decoded:
            subchunks.append(
                Chunk(
                    idx=chunk.idx,
                    header=chunk.header,
                    header_path=chunk.header_path,
                    content=decoded,
                    token_count=len(chunk_ids),
                    subindex=subindex,
                )
            )
            subindex += 1
        if cur_idx == len(input_ids):
            break
        idx += tokens_per_chunk
    return subchunks


def process_chunk(conn: psycopg.Connection, page: Page, chunk: Chunk) -> None:
    if chunk.content == "":  # discard empty chunks
        return

    chunk.token_count = len(ENC.encode(chunk.content))
    if chunk.token_count < 10:  # discard chunks that are too tiny to be useful
        return

    chunks = [chunk]

    if chunk.token_count > MAX_CHUNK_TOKENS:
        print(
            f"Chunk {chunk.header} too large ({chunk.token_count} tokens), splitting..."
        )
        chunks = split_chunk(chunk)

    for chunk in chunks:
        insert_chunk(conn, page, chunk)
    conn.commit()


def chunk_files(conn: psycopg.Connection, version: int) -> None:
    conn.execute(
        SQL("create table {table} (like docs.postgres_pages including all)").format(
            table=TMP_PAGES_TABLE
        )
    )
    conn.execute(
        SQL(
            "insert into {table} select * from docs.postgres_pages where version != %s"
        ).format(table=TMP_PAGES_TABLE),
        [version],
    )
    conn.execute(
        SQL("create table {table} (like docs.postgres_chunks including all)").format(
            table=TMP_CHUNKS_TABLE
        )
    )
    conn.execute(
        SQL(
            "insert into {table} select c.* from docs.postgres_chunks c inner join docs.postgres_pages p on c.page_id = p.id where p.version != %s"
        ).format(table=TMP_CHUNKS_TABLE),
        [version],
    )
    conn.execute(
        SQL(
            "alter table {chunks_table} add foreign key (page_id) references {pages_table}(id) on delete cascade"
        ).format(chunks_table=TMP_CHUNKS_TABLE, pages_table=TMP_PAGES_TABLE)
    )
    conn.commit()

    header_pattern = re.compile("^(#{1,3}) .+$")
    codeblock_pattern = re.compile("^```")

    section_prefix = r"^[A-Za-z0-9.]+\.\s*"
    chapter_prefix = r"^Chapter\s+[0-9]+\.\s*"

    page_count = 0

    for md in MD_DIR.glob("*.md"):
        print(f"chunking {md}...")
        with md.open() as f:
            # process the frontmatter
            f.readline()
            f.readline()  # title line
            slug = f.readline().split(":", 1)[1].strip()
            refentry = f.readline().split(":", 1)[1].strip().lower() == "true"
            f.readline()

            page = Page(
                id=0,
                version=version,
                url=f"{POSTGRES_BASE_URL}/{version}/{slug}",
                domain="postgresql.org",
                filename=md.name,
            )
            page_count += 1

            insert_page(conn, page)

            header_path = []
            idx = 0
            chunk: Chunk | None = None
            in_codeblock = False
            while True:
                line = f.readline()
                if line == "":
                    if chunk is not None:
                        process_chunk(conn, page, chunk)
                    break
                match = header_pattern.match(line)
                if match is None or in_codeblock or (refentry and chunk is not None):
                    assert chunk is not None
                    if codeblock_pattern.match(line):
                        in_codeblock = not in_codeblock
                    chunk.content += line
                    continue
                header_hases = match.group(1)
                depth = len(header_hases)
                header_path = header_path[: (depth - 1)]
                header = line.lstrip("#").strip()
                header = re.sub(section_prefix, "", header).strip()
                header = re.sub(chapter_prefix, "", header).strip()
                header_path.append(header)
                if chunk is not None:
                    process_chunk(conn, page, chunk)
                chunk = Chunk(
                    idx=idx,
                    header=header,
                    header_path=header_path.copy(),
                    content="",
                )
                idx += 1
            update_page_stats(conn, page)
            conn.commit()

    with conn.cursor() as cur:
        cur.execute("drop table docs.postgres_chunks")
        cur.execute("drop table docs.postgres_pages")
        cur.execute(
            SQL("alter table {table} rename to postgres_chunks").format(
                table=TMP_CHUNKS_TABLE
            )
        )
        cur.execute(
            SQL("alter table {table} rename to postgres_pages").format(
                table=TMP_PAGES_TABLE
            )
        )
        conn.commit()

    print(f"Processed {page_count} pages.")


def main():
    parser = argparse.ArgumentParser(
        description="Ingest Postgres documentation into the database."
    )
    parser.add_argument("version", type=int, help="Postgres version to ingest")
    args = parser.parse_args()
    version = args.version
    update_repo()
    tag = get_version_tag(version)
    db_uri = f"postgresql://{os.environ['PGUSER']}:{os.environ['PGPASSWORD']}@{os.environ['PGHOST']}:{os.environ['PGPORT']}/{os.environ['PGDATABASE']}"
    with psycopg.connect(db_uri) as conn:
        print(f"Building Postgres {version} ({tag}) documentation...")
        checkout_tag(tag)
        build_html()
        build_markdown()
        try:
            chunk_files(conn, version)
        except Exception as e:
            with conn.cursor() as cur:
                cur.execute(
                    SQL("drop table if exists {table}").format(table=TMP_CHUNKS_TABLE)
                )
                cur.execute(
                    SQL("drop table if exists {table}").format(table=TMP_PAGES_TABLE)
                )
                conn.commit()
            raise e


if __name__ == "__main__":
    main()
